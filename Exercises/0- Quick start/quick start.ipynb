{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Importing necessary libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- loading MNIST (train+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    train=True, \n",
    "    root='data', \n",
    "    download=True, \n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    train=False, \n",
    "    root='data', \n",
    "    download=True, \n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Creating data loaders to tranform the data into batches iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size -> 32\n",
      "#channels -> 1\n",
      "height -> 28\n",
      "width -> 28\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for x, y in test_dataloader:\n",
    "    print(f\"batch size -> {x.shape[0]}\\n#channels -> {x.shape[1]}\\nheight -> {x.shape[2]}\\nwidth -> {x.shape[3]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Defining a simple four layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (four_layer_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.four_layer_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 256), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(256, 128), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128,64), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.four_layer_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training loop....\n",
    "def train(dataloader, model, optimizer, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        X, y = x.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch %100 == 0:\n",
    "            current_loss, current_batch = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss -> {current_loss:>8f} , batch -> [{current_batch:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing loop.....\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            loss += loss_fn(preds, y).item()\n",
    "            correct += (preds.argmax(1)==y).type(torch.float).sum().item()\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error:\\nloss -> {loss:>8f}\\t Accuracy -> {correct*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- training and evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "----------------\n",
      "loss -> 2.308447 , batch -> [   32/60000]\n",
      "loss -> 0.504153 , batch -> [ 3232/60000]\n",
      "loss -> 0.487113 , batch -> [ 6432/60000]\n",
      "loss -> 0.207600 , batch -> [ 9632/60000]\n",
      "loss -> 0.269628 , batch -> [12832/60000]\n",
      "loss -> 0.270383 , batch -> [16032/60000]\n",
      "loss -> 0.164871 , batch -> [19232/60000]\n",
      "loss -> 0.102817 , batch -> [22432/60000]\n",
      "loss -> 0.138588 , batch -> [25632/60000]\n",
      "loss -> 0.161988 , batch -> [28832/60000]\n",
      "loss -> 0.320695 , batch -> [32032/60000]\n",
      "loss -> 0.229939 , batch -> [35232/60000]\n",
      "loss -> 0.155359 , batch -> [38432/60000]\n",
      "loss -> 0.206136 , batch -> [41632/60000]\n",
      "loss -> 0.114463 , batch -> [44832/60000]\n",
      "loss -> 0.159835 , batch -> [48032/60000]\n",
      "loss -> 0.176960 , batch -> [51232/60000]\n",
      "loss -> 0.063046 , batch -> [54432/60000]\n",
      "loss -> 0.145418 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.142647\t Accuracy -> 95.64\n",
      "epoch 2\n",
      "----------------\n",
      "loss -> 0.051772 , batch -> [   32/60000]\n",
      "loss -> 0.276330 , batch -> [ 3232/60000]\n",
      "loss -> 0.090728 , batch -> [ 6432/60000]\n",
      "loss -> 0.055455 , batch -> [ 9632/60000]\n",
      "loss -> 0.156731 , batch -> [12832/60000]\n",
      "loss -> 0.106331 , batch -> [16032/60000]\n",
      "loss -> 0.093406 , batch -> [19232/60000]\n",
      "loss -> 0.026147 , batch -> [22432/60000]\n",
      "loss -> 0.108890 , batch -> [25632/60000]\n",
      "loss -> 0.042846 , batch -> [28832/60000]\n",
      "loss -> 0.142215 , batch -> [32032/60000]\n",
      "loss -> 0.227546 , batch -> [35232/60000]\n",
      "loss -> 0.071742 , batch -> [38432/60000]\n",
      "loss -> 0.098333 , batch -> [41632/60000]\n",
      "loss -> 0.090676 , batch -> [44832/60000]\n",
      "loss -> 0.085529 , batch -> [48032/60000]\n",
      "loss -> 0.055131 , batch -> [51232/60000]\n",
      "loss -> 0.008380 , batch -> [54432/60000]\n",
      "loss -> 0.142740 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.118277\t Accuracy -> 96.50999999999999\n",
      "epoch 3\n",
      "----------------\n",
      "loss -> 0.023714 , batch -> [   32/60000]\n",
      "loss -> 0.110748 , batch -> [ 3232/60000]\n",
      "loss -> 0.012999 , batch -> [ 6432/60000]\n",
      "loss -> 0.059392 , batch -> [ 9632/60000]\n",
      "loss -> 0.089871 , batch -> [12832/60000]\n",
      "loss -> 0.069217 , batch -> [16032/60000]\n",
      "loss -> 0.053874 , batch -> [19232/60000]\n",
      "loss -> 0.006432 , batch -> [22432/60000]\n",
      "loss -> 0.190632 , batch -> [25632/60000]\n",
      "loss -> 0.013635 , batch -> [28832/60000]\n",
      "loss -> 0.279480 , batch -> [32032/60000]\n",
      "loss -> 0.133841 , batch -> [35232/60000]\n",
      "loss -> 0.071697 , batch -> [38432/60000]\n",
      "loss -> 0.024472 , batch -> [41632/60000]\n",
      "loss -> 0.054569 , batch -> [44832/60000]\n",
      "loss -> 0.055337 , batch -> [48032/60000]\n",
      "loss -> 0.016762 , batch -> [51232/60000]\n",
      "loss -> 0.002216 , batch -> [54432/60000]\n",
      "loss -> 0.067490 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.124921\t Accuracy -> 96.37\n",
      "epoch 4\n",
      "----------------\n",
      "loss -> 0.127627 , batch -> [   32/60000]\n",
      "loss -> 0.034540 , batch -> [ 3232/60000]\n",
      "loss -> 0.000950 , batch -> [ 6432/60000]\n",
      "loss -> 0.017024 , batch -> [ 9632/60000]\n",
      "loss -> 0.023821 , batch -> [12832/60000]\n",
      "loss -> 0.057548 , batch -> [16032/60000]\n",
      "loss -> 0.059246 , batch -> [19232/60000]\n",
      "loss -> 0.009088 , batch -> [22432/60000]\n",
      "loss -> 0.046170 , batch -> [25632/60000]\n",
      "loss -> 0.004910 , batch -> [28832/60000]\n",
      "loss -> 0.154802 , batch -> [32032/60000]\n",
      "loss -> 0.023876 , batch -> [35232/60000]\n",
      "loss -> 0.032353 , batch -> [38432/60000]\n",
      "loss -> 0.027138 , batch -> [41632/60000]\n",
      "loss -> 0.011898 , batch -> [44832/60000]\n",
      "loss -> 0.036945 , batch -> [48032/60000]\n",
      "loss -> 0.040180 , batch -> [51232/60000]\n",
      "loss -> 0.002960 , batch -> [54432/60000]\n",
      "loss -> 0.025428 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.094078\t Accuracy -> 97.54\n",
      "epoch 5\n",
      "----------------\n",
      "loss -> 0.014941 , batch -> [   32/60000]\n",
      "loss -> 0.012155 , batch -> [ 3232/60000]\n",
      "loss -> 0.002396 , batch -> [ 6432/60000]\n",
      "loss -> 0.008009 , batch -> [ 9632/60000]\n",
      "loss -> 0.020992 , batch -> [12832/60000]\n",
      "loss -> 0.054040 , batch -> [16032/60000]\n",
      "loss -> 0.014598 , batch -> [19232/60000]\n",
      "loss -> 0.096825 , batch -> [22432/60000]\n",
      "loss -> 0.008367 , batch -> [25632/60000]\n",
      "loss -> 0.001357 , batch -> [28832/60000]\n",
      "loss -> 0.102245 , batch -> [32032/60000]\n",
      "loss -> 0.007636 , batch -> [35232/60000]\n",
      "loss -> 0.034352 , batch -> [38432/60000]\n",
      "loss -> 0.009866 , batch -> [41632/60000]\n",
      "loss -> 0.036616 , batch -> [44832/60000]\n",
      "loss -> 0.020869 , batch -> [48032/60000]\n",
      "loss -> 0.027551 , batch -> [51232/60000]\n",
      "loss -> 0.001285 , batch -> [54432/60000]\n",
      "loss -> 0.028609 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.137089\t Accuracy -> 96.41999999999999\n",
      "epoch 6\n",
      "----------------\n",
      "loss -> 0.003802 , batch -> [   32/60000]\n",
      "loss -> 0.022325 , batch -> [ 3232/60000]\n",
      "loss -> 0.276340 , batch -> [ 6432/60000]\n",
      "loss -> 0.000906 , batch -> [ 9632/60000]\n",
      "loss -> 0.165489 , batch -> [12832/60000]\n",
      "loss -> 0.024905 , batch -> [16032/60000]\n",
      "loss -> 0.005241 , batch -> [19232/60000]\n",
      "loss -> 0.000229 , batch -> [22432/60000]\n",
      "loss -> 0.003178 , batch -> [25632/60000]\n",
      "loss -> 0.000204 , batch -> [28832/60000]\n",
      "loss -> 0.198392 , batch -> [32032/60000]\n",
      "loss -> 0.002522 , batch -> [35232/60000]\n",
      "loss -> 0.025596 , batch -> [38432/60000]\n",
      "loss -> 0.019288 , batch -> [41632/60000]\n",
      "loss -> 0.011306 , batch -> [44832/60000]\n",
      "loss -> 0.001974 , batch -> [48032/60000]\n",
      "loss -> 0.034550 , batch -> [51232/60000]\n",
      "loss -> 0.130845 , batch -> [54432/60000]\n",
      "loss -> 0.011388 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.090138\t Accuracy -> 97.86\n",
      "epoch 7\n",
      "----------------\n",
      "loss -> 0.012868 , batch -> [   32/60000]\n",
      "loss -> 0.024120 , batch -> [ 3232/60000]\n",
      "loss -> 0.000182 , batch -> [ 6432/60000]\n",
      "loss -> 0.002937 , batch -> [ 9632/60000]\n",
      "loss -> 0.152495 , batch -> [12832/60000]\n",
      "loss -> 0.010154 , batch -> [16032/60000]\n",
      "loss -> 0.009294 , batch -> [19232/60000]\n",
      "loss -> 0.000779 , batch -> [22432/60000]\n",
      "loss -> 0.026234 , batch -> [25632/60000]\n",
      "loss -> 0.001235 , batch -> [28832/60000]\n",
      "loss -> 0.035119 , batch -> [32032/60000]\n",
      "loss -> 0.012840 , batch -> [35232/60000]\n",
      "loss -> 0.025540 , batch -> [38432/60000]\n",
      "loss -> 0.066650 , batch -> [41632/60000]\n",
      "loss -> 0.055724 , batch -> [44832/60000]\n",
      "loss -> 0.026614 , batch -> [48032/60000]\n",
      "loss -> 0.015059 , batch -> [51232/60000]\n",
      "loss -> 0.000104 , batch -> [54432/60000]\n",
      "loss -> 0.003253 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.112539\t Accuracy -> 97.28999999999999\n",
      "epoch 8\n",
      "----------------\n",
      "loss -> 0.010051 , batch -> [   32/60000]\n",
      "loss -> 0.017318 , batch -> [ 3232/60000]\n",
      "loss -> 0.000858 , batch -> [ 6432/60000]\n",
      "loss -> 0.081500 , batch -> [ 9632/60000]\n",
      "loss -> 0.007704 , batch -> [12832/60000]\n",
      "loss -> 0.122162 , batch -> [16032/60000]\n",
      "loss -> 0.031629 , batch -> [19232/60000]\n",
      "loss -> 0.000735 , batch -> [22432/60000]\n",
      "loss -> 0.000280 , batch -> [25632/60000]\n",
      "loss -> 0.002163 , batch -> [28832/60000]\n",
      "loss -> 0.049536 , batch -> [32032/60000]\n",
      "loss -> 0.001533 , batch -> [35232/60000]\n",
      "loss -> 0.021081 , batch -> [38432/60000]\n",
      "loss -> 0.002439 , batch -> [41632/60000]\n",
      "loss -> 0.009115 , batch -> [44832/60000]\n",
      "loss -> 0.003018 , batch -> [48032/60000]\n",
      "loss -> 0.010032 , batch -> [51232/60000]\n",
      "loss -> 0.000887 , batch -> [54432/60000]\n",
      "loss -> 0.011466 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.085725\t Accuracy -> 97.87\n",
      "epoch 9\n",
      "----------------\n",
      "loss -> 0.002770 , batch -> [   32/60000]\n",
      "loss -> 0.010608 , batch -> [ 3232/60000]\n",
      "loss -> 0.038040 , batch -> [ 6432/60000]\n",
      "loss -> 0.030531 , batch -> [ 9632/60000]\n",
      "loss -> 0.013840 , batch -> [12832/60000]\n",
      "loss -> 0.012077 , batch -> [16032/60000]\n",
      "loss -> 0.000688 , batch -> [19232/60000]\n",
      "loss -> 0.030370 , batch -> [22432/60000]\n",
      "loss -> 0.022332 , batch -> [25632/60000]\n",
      "loss -> 0.000225 , batch -> [28832/60000]\n",
      "loss -> 0.002950 , batch -> [32032/60000]\n",
      "loss -> 0.001056 , batch -> [35232/60000]\n",
      "loss -> 0.003710 , batch -> [38432/60000]\n",
      "loss -> 0.000595 , batch -> [41632/60000]\n",
      "loss -> 0.002699 , batch -> [44832/60000]\n",
      "loss -> 0.002112 , batch -> [48032/60000]\n",
      "loss -> 0.005103 , batch -> [51232/60000]\n",
      "loss -> 0.000228 , batch -> [54432/60000]\n",
      "loss -> 0.039173 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.125167\t Accuracy -> 96.93\n",
      "epoch 10\n",
      "----------------\n",
      "loss -> 0.016940 , batch -> [   32/60000]\n",
      "loss -> 0.006038 , batch -> [ 3232/60000]\n",
      "loss -> 0.056828 , batch -> [ 6432/60000]\n",
      "loss -> 0.000204 , batch -> [ 9632/60000]\n",
      "loss -> 0.025906 , batch -> [12832/60000]\n",
      "loss -> 0.190647 , batch -> [16032/60000]\n",
      "loss -> 0.008762 , batch -> [19232/60000]\n",
      "loss -> 0.000252 , batch -> [22432/60000]\n",
      "loss -> 0.013986 , batch -> [25632/60000]\n",
      "loss -> 0.000184 , batch -> [28832/60000]\n",
      "loss -> 0.032279 , batch -> [32032/60000]\n",
      "loss -> 0.002192 , batch -> [35232/60000]\n",
      "loss -> 0.006950 , batch -> [38432/60000]\n",
      "loss -> 0.006589 , batch -> [41632/60000]\n",
      "loss -> 0.004092 , batch -> [44832/60000]\n",
      "loss -> 0.003431 , batch -> [48032/60000]\n",
      "loss -> 0.017142 , batch -> [51232/60000]\n",
      "loss -> 0.000133 , batch -> [54432/60000]\n",
      "loss -> 0.039023 , batch -> [57632/60000]\n",
      "Test Error:\n",
      "loss -> 0.101700\t Accuracy -> 97.78999999999999\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print(f\"epoch {i+1}\\n----------------\")\n",
    "    train(train_dataloader, model, optimizer, loss_fn)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simple_nn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('simple_nn_model.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (four_layer_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- testing the loaded model on a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 7 - seven\t actual: 7 - seven\n"
     ]
    }
   ],
   "source": [
    "classes = training_data.classes\n",
    "\n",
    "model.eval()\n",
    "X, y = test_data[0][0], test_data[0][1]\n",
    "pred = model(X)\n",
    "prediction ,actual= classes[pred[0].argmax(0)], classes[y]\n",
    "print(f\"prediction: {prediction}\\t actual: {actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
